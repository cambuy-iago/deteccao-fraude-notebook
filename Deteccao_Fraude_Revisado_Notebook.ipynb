{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc6243f",
   "metadata": {},
   "source": [
    "# Detecção de Fraude em Cartões de Crédito - Notebook Organizacional\n",
    "## Introdução\n",
    "**Objetivo:**\n",
    "Neste notebook, iremos construir um projeto completo de detecção de fraudes em transações de cartão de crédito utilizando técnicas de Deep Learning. O foco é explicar conceitos de forma didática, aplicando boas práticas de projeto.\n",
    "\n",
    "**Por que é importante?**\n",
    "Fraudes em cartões de crédito geram prejuízos significativos. Modelos de detecção automatizados auxiliam a identificar transações suspeitas antes que causem danos.\n",
    "\n",
    "**Como funciona em termos gerais?**\n",
    "A ideia central é treinar uma rede neural para aprender padrões presentes em transações legítimas e fraudulentas. Cada transação possui diversas características (features) que alimentam o modelo. Assim como um filtro que aprende a separar materiais de acordo com suas propriedades, a rede neural refina as informações em camadas, ajustando pesos para classificar corretamente.\n",
    "\n",
    "### Estrutura do Notebook\n",
    "1. Importações e Verificação de Versões\n",
    "2. Carregamento e Exploração dos Dados (EDA)\n",
    "3. Pré-processamento: Divisão, Padronização e Tratamento de Desbalanceamento\n",
    "4. Definição da Arquitetura da Rede Neural\n",
    "5. Treinamento com Callbacks e Class Weights\n",
    "6. Avaliação do Modelo: Métricas e Visualizações\n",
    "7. Função de Previsão para Novas Transações\n",
    "8. Exportação de Modelo e Scaler\n",
    "9. Conclusões e Próximos Passos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91252ed8",
   "metadata": {},
   "source": [
    "## 1. Importações e Verificação de Versões\n",
    "**O que será feito:**\n",
    "Importaremos bibliotecas essenciais para manipulação de dados, visualizações e construção do modelo.\n",
    "\n",
    "**Por que é importante?**\n",
    "Verificar versões garante compatibilidade e reprodutibilidade do projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas essenciais\n",
    "import sys  # Para verificar versão do Python\n",
    "import pandas as pd  # Manipulação de dados\n",
    "import numpy as np  # Operações numéricas\n",
    "import matplotlib.pyplot as plt  # Visualizações básicas\n",
    "import seaborn as sns  # Visualizações avançadas\n",
    "import tensorflow as tf  # Framework de Deep Learning\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, RocCurveDisplay,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE  # Tratamento de desbalanceamento\n",
    "import joblib  # Salvamento de objetos Python\n",
    "import sklearn  # Biblioteca de aprendizado de máquina\n",
    "\n",
    "# Exibindo versões para garantia de reprodutibilidade\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"imbalanced-learn:\", SMOTE.__module__.split('.')[0])\n",
    "\n",
    "# Configurando estilo das figuras\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Semente para reprodutibilidade\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d9066",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Exploração dos Dados (EDA)\n",
    "**O que será feito:**\n",
    "1. Leremos o arquivo CSV com as transações de cartão de crédito.\n",
    "2. Faremos uma análise exploratória inicial, visualizando a distribuição das classes e estatísticas descritivas.\n",
    "\n",
    "**Por que é importante?**\n",
    "Compreender a estrutura dos dados e o grau de desbalanceamento é crucial para planejar o modelo e as estratégias de pré-processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Leitura do CSV\n",
    "df = pd.read_csv('creditcard.csv')  # Assegure-se de ter enviado o arquivo para o Colab\n",
    "print(\"Formato dos dados:\", df.shape)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045101bf",
   "metadata": {},
   "source": [
    "### 2.2. Distribuição da Classe (Fraude vs. Não-Fraude)\n",
    "**O que será feito:**\n",
    "Visualizar quantidade de transações legítimas e fraudulentas. Calcular proporção.\n",
    "\n",
    "**Por que é importante?**\n",
    "O dataset é altamente desbalanceado, e entender essa proporção ajuda a decidir técnicas de tratamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da distribuição das classes\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title(\"Distribuição entre Transações Legítimas (0) e Fraudulentas (1)\")\n",
    "plt.xticks([0,1], ['Legítima', 'Fraude'])\n",
    "plt.show()\n",
    "\n",
    "# Proporção de cada classe\n",
    "proporcao = df['Class'].value_counts(normalize=True)\n",
    "print(\"Proporção de cada classe:\\n\", proporcao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513d597",
   "metadata": {},
   "source": [
    "### 2.3. Estatísticas Descritivas de ‘Time’ e ‘Amount’ e Visualizações\n",
    "**O que será feito:**\n",
    "Exibir estatísticas descritivas e analisar distribuições de ‘Amount’. Visualizar boxplot e histograma.\n",
    "\n",
    "**Por que é importante?**\n",
    "Variáveis como ‘Amount’ podem ter distribuições assimétricas e outliers que impactam o modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b418f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas descritivas de ‘Time’ e ‘Amount’\n",
    "df[['Time','Amount']].describe().round(2)\n",
    "\n",
    "# Histograma de 'Amount'\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df['Amount'], bins=50, kde=True)\n",
    "plt.title(\"Distribuição do Valor (‘Amount’)\")\n",
    "plt.xlabel(\"Montante (em USD)\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot de 'Amount' por Classe (escala log para legibilidade)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='Class', y='Amount', data=df)\n",
    "plt.yscale('log')  # Torna a escala mais legível para outliers\n",
    "plt.title(\"Boxplot de Montante por Classe\")\n",
    "plt.xticks([0,1], ['Legítima', 'Fraude'])\n",
    "plt.show()\n",
    "\n",
    "# Resumo de um subconjunto de features PCA (V1 a V28) para ilustração\n",
    "df[['V1','V2','V3','V28']].describe().round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161379e",
   "metadata": {},
   "source": [
    "## 3. Pré-processamento: Divisão, Padronização e Tratamento de Desbalanceamento\n",
    "### 3.1. Definir X e y\n",
    "**O que será feito:**\n",
    "Separar features (`X`) e variável alvo (`y`). Verificar e remover possíveis NaNs em `y`.\n",
    "\n",
    "**Por que é importante?**\n",
    "Garantir que não existam valores ausentes na variável alvo evita erros no treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f925f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando features e target\n",
    "X = df.drop('Class', axis=1).copy()\n",
    "y = df['Class'].copy()\n",
    "\n",
    "# Verificar e remover linhas com NaN em y (precaução)\n",
    "if y.isnull().any():\n",
    "    print(\"NaN values found in y. Removing corresponding rows.\")\n",
    "    nan_indices = y[y.isnull()].index\n",
    "    X = X.drop(nan_indices)\n",
    "    y = y.drop(nan_indices)\n",
    "    print(f\"Removed {len(nan_indices)} rows containing NaN in y.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff92b9",
   "metadata": {},
   "source": [
    "### 3.2. Divisão em Treino e Teste (com Stratify)\n",
    "**O que será feito:**\n",
    "Dividiremos o dataset em 80% treino e 20% teste, mantendo a proporção das classes (`stratify`).\n",
    "\n",
    "**Por que é importante?**\n",
    "Manter a proporção de classes em ambos conjuntos evita viés na avaliação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9053508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "print(\"Shape treino:\", X_train.shape, \"Shape teste:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d754a61",
   "metadata": {},
   "source": [
    "### 3.3. Padronizar Features Numéricas\n",
    "**O que será feito:**\n",
    "Padronizar todos os atributos (incluindo ‘Time’ e ‘Amount’) para melhorar a convergência do modelo.\n",
    "\n",
    "**Por que é importante?**\n",
    "Redes neurais são sensíveis às escalas das features; normalizar ajuda no treinamento estável.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5423188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar e transformar o conjunto de treino; transformar o conjunto de teste\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Salvando scaler para uso posterior\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Verificação rápida das estatísticas após padronização\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "print(\"Média (treino):\", X_train_scaled_df.mean().round(2).head())\n",
    "print(\"Std (treino):\", X_train_scaled_df.std().round(2).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8a6d4",
   "metadata": {},
   "source": [
    "### 3.4. Análise do Desbalanceamento\n",
    "**O que será feito:**\n",
    "Contar transações legítimas vs. fraudulentas no conjunto de treino.\n",
    "\n",
    "**Por que é importante?**\n",
    "Taxa de fraude muito baixa (cerca de 0.2%) exige estratégias específicas para evitar que o modelo aprenda apenas a classe majoritária.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagem de classes no conjunto de treino\n",
    "contagem = y_train.value_counts()\n",
    "print(\"Contagem no treino:\\n\", contagem)\n",
    "print(f\"Taxa de fraude no treino: {contagem[1]/(contagem[0]+contagem[1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d44e3",
   "metadata": {},
   "source": [
    "### 3.5. Estratégias para Lidar com Desbalanceamento\n",
    "**Opções abordadas:**\n",
    "1. `class_weight`: penalizar mais os erros na classe minoritária.\n",
    "2. `SMOTE`: gerar amostras sintéticas da classe minoritária.\n",
    "\n",
    "**Por que é importante?**\n",
    "Em problemas de fraude, queremos maximizar a detecção de fraudes (alta `recall` para classe 1), sem gerar muitos falsos positivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5.1. Cálculo de class_weight inversamente proporcional à frequência\n",
    "weight_for_0 = (1 / contagem[0]) * (len(y_train) / 2.0)\n",
    "weight_for_1 = (1 / contagem[1]) * (len(y_train) / 2.0)\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# 3.5.2. Oversampling com SMOTE (opcional)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "print(\"Após SMOTE, nova contagem:\", pd.Series(y_train_res).value_counts())\n",
    "\n",
    "# Observação: Escolha entre usar class_weight ou SMOTE. Comparar resultados posteriormente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18e04b",
   "metadata": {},
   "source": [
    "## 4. Definição da Arquitetura da Rede Neural (Keras)\n",
    "### 4.1. Configuração do Modelo\n",
    "**O que será feito:**\n",
    "Definiremos um modelo `Sequential` com camadas `Dense` e `Dropout`. Incluiremos uma camada adicional para representação intermediária.\n",
    "\n",
    "**Por que é importante?**\n",
    "Arquitetura de rede define a capacidade do modelo em aprender padrões complexos. Camadas `Dropout` ajudam a prevenir overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo dimensão de entrada\n",
    "input_dim = X_train_scaled.shape[1]  # Número de features\n",
    "\n",
    "# Construindo o modelo\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(input_dim,)),  # Camada de entrada explicita\n",
    "    Dense(32, activation='relu'),  # Primeira camada oculta com 32 neurônios\n",
    "    Dropout(0.3),  # Dropout de 30% para reduzir overfitting\n",
    "    Dense(16, activation='relu'),  # Segunda camada oculta com 16 neurônios\n",
    "    Dropout(0.2),  # Dropout de 20%\n",
    "    Dense(8, activation='relu'),   # Terceira camada oculta com 8 neurônios\n",
    "    Dropout(0.2),  # Dropout de 20%\n",
    "    Dense(1, activation='sigmoid')  # Camada de saída para classificação binária\n",
    "])\n",
    "\n",
    "# Compilando o modelo com métricas apropriadas para desbalanceamento\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')  # AUC-ROC\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Exibindo resumo da arquitetura\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd39b9",
   "metadata": {},
   "source": [
    "## 5. Treinamento com Callbacks e Class Weights\n",
    "### 5.1. Configuração de Callbacks\n",
    "**O que será feito:**\n",
    "Definiremos `EarlyStopping` para evitar overfitting e `ModelCheckpoint` para salvar o melhor modelo conforme AUC.\n",
    "\n",
    "**Por que é importante?**\n",
    "`EarlyStopping` interrompe o treino quando não há melhoria, economizando recursos. `ModelCheckpoint` armazena pesos do melhor modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # Para se não houver melhoria em 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "chkpt = ModelCheckpoint(\n",
    "    'best_fraud_model.h5',\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b3205",
   "metadata": {},
   "source": [
    "### 5.2. Treinamento do Modelo\n",
    "**O que será feito:**\n",
    "Treinaremos usando `class_weight` para penalizar a classe minoritária. Caso prefira SMOTE, basta alterar para `(X_train_res, y_train_res)` e remover `class_weight`.\n",
    "\n",
    "**Por que é importante?**\n",
    "Garantir que o modelo não ignore fraudes, valorizando erros em transações fraudulentas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo com class weights\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,  # 20% do treino para validação\n",
    "    epochs=50,\n",
    "    batch_size=2048,\n",
    "    class_weight=class_weights,  # Penalização para classe minoritária\n",
    "    callbacks=[early_stop, chkpt],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Se preferir usar SMOTE, comente o bloco acima e use o abaixo:\n",
    "# history = model.fit(\n",
    "#     X_train_res, y_train_res,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=2048,\n",
    "#     callbacks=[early_stop, chkpt],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# Carregando o melhor modelo salvo\n",
    "best_model = load_model('best_fraud_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808effc",
   "metadata": {},
   "source": [
    "## 6. Avaliação do Modelo: Métricas e Visualizações\n",
    "**O que será feito:**\n",
    "1. Predições no conjunto de teste.\n",
    "2. Matriz de confusão.\n",
    "3. Relatório de classificação (precision, recall, f1-score).\n",
    "4. Curvas ROC AUC e Precision-Recall.\n",
    "\n",
    "**Por que é importante?**\n",
    "Avaliar o modelo em dados nunca vistos mostra sua generalização e ajuda a entender trade-offs em desbalanceamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1. Predições no conjunto de teste\n",
    "y_prob = best_model.predict(X_test_scaled, batch_size=2048).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# 6.2. Matriz de Confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Legítima (0)', 'Fraude (1)'],\n",
    "    yticklabels=['Legítima (0)', 'Fraude (1)']\n",
    ")\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"Matriz de Confusão (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# 6.3. Relatório de Classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# 6.4. ROC AUC e Curva ROC\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"AUC-ROC (Test): {auc_roc:.4f}\")\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "plt.title(\"Curva ROC (Test)\")\n",
    "plt.show()\n",
    "\n",
    "# 6.5. Precision-Recall Curve e AUC-PR\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "auc_pr = auc(recall, precision)\n",
    "print(f\"AUC-PR (Test): {auc_pr:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.4f}')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Curva Precision-Recall (Test)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3b0c1",
   "metadata": {},
   "source": [
    "## 7. Função de Previsão para Novas Transações\n",
    "**O que será feito:**\n",
    "Definiremos uma função que recebe um array de features de uma nova transação e retorna probabilidade e classificação com base em um threshold.\n",
    "\n",
    "**Por que é importante?**\n",
    "Em produção, precisamos de uma forma simples de usar o modelo treinado em transações ao vivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_transaction(values_array, scaler, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Recebe: \n",
    "      - values_array: array ou lista com 30 features na mesma ordem do dataframe original\n",
    "      - scaler: objeto StandardScaler ajustado nos dados de treino\n",
    "      - model: modelo Keras carregado\n",
    "      - threshold: ponto de corte para classificar como fraude (padrão 0.5)\n",
    "    Retorna:\n",
    "      - dicionário com probabilidade e label ('Fraude' ou 'Não Fraude')\n",
    "    \"\"\"\n",
    "    # Convertendo para numpy array e redimensionando\n",
    "    arr = np.array(values_array).reshape(1, -1)  # (1, 30)\n",
    "    # Padronizando os valores\n",
    "    arr_scaled = scaler.transform(arr)\n",
    "    # Obtendo probabilidade de fraude\n",
    "    prob = float(model.predict(arr_scaled).ravel())\n",
    "    # Definindo rótulo com base no threshold\n",
    "    label = \"Fraude\" if prob >= threshold else \"Não Fraude\"\n",
    "    return {\"probabilidade\": prob, \"label\": label}\n",
    "\n",
    "# Exemplo de uso da função (usamos a primeira transação de teste)\n",
    "exemplo = X_test.iloc[0].values\n",
    "resultado = predict_transaction(exemplo, scaler, best_model)\n",
    "print(f\"Probabilidade: {resultado['probabilidade']:.4f}\")\n",
    "print(f\"Classificação: {resultado['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a319cc",
   "metadata": {},
   "source": [
    "## 8. Exportação de Modelo e Scaler\n",
    "**O que será feito:**\n",
    "Salvaremos o scaler e o modelo final para uso em produção.\n",
    "\n",
    "**Por que é importante?**\n",
    "Permite carregar rapidamente o modelo escalado em aplicações sem necessidade de retrinar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O modelo já foi salvo pelo ModelCheckpoint em 'best_fraud_model.h5'\n",
    "print(\"Modelo salvo como 'best_fraud_model.h5'.\")\n",
    "\n",
    "# Salvando o scaler (já realizado anteriormente, mas reforçando)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Scaler salvo como 'scaler.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2020b",
   "metadata": {},
   "source": [
    "## 9. Conclusões e Próximos Passos\n",
    "**Conclusões:**\n",
    "- Utilizamos uma abordagem de rede neural com camadas densas e dropout, obtendo bom desempenho ao priorizar métricas como AUC, precision e recall.\n",
    "- O uso de `class_weight` e `SMOTE` foram estratégias eficazes para tratar o desbalanceamento, mas precisam ser comparadas empiricamente.\n",
    "- Ferramentas como `EarlyStopping` e `ModelCheckpoint` garantem melhor controle sobre o treinamento.\n",
    "\n",
    "**Próximos Passos:**\n",
    "- Testar arquiteturas mais avançadas, como Autoencoders para detecção de anomalias.\n",
    "- Implementar pipelines com `tf.data.Dataset` para melhor performance em produção.\n",
    "- Realizar tuning de hiperparâmetros com `GridSearchCV` ou `RandomizedSearchCV`.\n",
    "- Documentar em um repositório GitHub com README explicativo, instruções de execução e referências.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
